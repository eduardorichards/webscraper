diff --git a/main.py b/main.py
index ffd7517..a6dddb9 100644
--- a/main.py
+++ b/main.py
@@ -10,23 +10,23 @@ from utils.sqlite_storage import SQLiteStorage
 
 def main():
     """Main function to run job scraping"""
-    
+
     scraper = JobScraper()
-    
+
     print("üéØ JOB SCRAPER STARTED")
     print("="*50)
-    
+
     search_config = SearchConfig(
-        keywords='python',           
-        location='Latin america',   
-        time_posted='24h',                 
-        remote=True,                     
-        experience_levels=[2],         
-        max_results=100    
+        keywords='python',
+        location='Latin america',
+        time_posted='24h',
+        remote=True,
+        experience_levels=[2],
+        max_results=100
     )
-    
+
     jobs = scraper.search_jobs(search_config, save_results=True)
-    
+
     if jobs:
         scraper.display_results(jobs)
     else:
@@ -36,48 +36,48 @@ def search_without_saving():
     """Example: Search without saving results (for testing)"""
     print("\nTESTING MODE - NO STORAGE")
     print("="*40)
-    
+
     scraper = JobScraper()
-    
+
     test_config = SearchConfig(
         keywords='java test',
         location='Argentina',
         experience_levels=[2, 3],
         max_results=5
     )
-    
+
     jobs = scraper.search_jobs(test_config, save_results=False)
-    
+
     print(f"Test search found {len(jobs)} jobs (not saved)")
-    
+
 def multiple_search():
     """Run multiple predefined searches"""
     from config.settings import SEARCH_TEMPLATES
-    
+
     scraper = JobScraper()
     total_jobs_found = 0
-    
+
     print("üöÄ STARTING BATCH SEARCH")
     print("="*50)
-    
+
     for i, template in enumerate(SEARCH_TEMPLATES, 1):
         search_name = template.pop("name", f"Search {i}")
         print(f"\nüìã Search {i}/{len(SEARCH_TEMPLATES)}: {search_name}")
         print("-" * 40)
-        
+
         search_config = SearchConfig(**template)
         jobs = scraper.search_jobs(search_config, save_results=True)
         total_jobs_found += len(jobs)
-        
+
         print(f"üìä Found {len(jobs)} jobs in this search")
-    
+
     # Show final statistics
     print(f"\n{'='*50}")
     print(f"üéØ BATCH SEARCH COMPLETE")
     print(f"{'='*50}")
     print(f"üìä Total jobs found: {total_jobs_found}")
     print(f"üìÅ Database file: data/jobs_master.db")
-    
+
     # Show DB statistics
     stats = scraper.sqlite_storage.get_stats()
     if stats:
@@ -85,23 +85,23 @@ def multiple_search():
         print(f"   Total unique jobs: {stats['total_jobs']}")
         print(f"   Unique companies: {stats['unique_companies']}")
         print(f"   Unique locations: {stats['unique_locations']}")
-    
+
 def parse_html():
         from bs4 import BeautifulSoup
         import requests
-        
+
         import os
-        
+
         url = 'https://www.linkedin.com/jobs/search/?keywords=Java&location=Argentina&geoId=100446943&f_E=2&f_TPR=r604800'
-        
+
         page = requests.get(url)
-        
+
         soup = BeautifulSoup(page.text, 'html')
-        
+
         soup = soup.prettify()
-        
+
         os.makedirs
-        
+
         with open('data/linkedin_page.html', 'w', encoding='utf-8') as f:
             f.write(soup)
 
@@ -193,7 +193,7 @@ def analyze_with_custom_keywords():
 
 if __name__ == "__main__":
     # Run batch job search
-    # multiple_search()
+    multiple_search()
 
     # OR analyze existing jobs for keywords
     analyze_keywords()
diff --git a/requirements.txt b/requirements.txt
index 010c7f0..1ee8a1b 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -4,4 +4,5 @@ lxml==4.9.3
 pandas==2.2.0
 fake-useragent==1.4.0
 python-dotenv==1.0.0
+flask==3.1.0
 selenium==4.15.0
